{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUJCBdgRlxYH"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ohP8lx_TK4p"
      },
      "source": [
        "#1.Library\r\n",
        "\r\n",
        "\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrD2QIst3Oj3"
      },
      "source": [
        "# from tensorflow.keras.datasets import cifar10"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0mGeHBw3gDC"
      },
      "source": [
        "# def load_cifar10():\r\n",
        "#     # load the data\r\n",
        "#     (x_train, y_train), (x_test, y_test) = cifar10.load_data()\r\n",
        "#     # normalize our inputs to be in the range[-1, 1]\r\n",
        "#     x_train = (x_train.astype(np.float32) - 127.5)/127.5\r\n",
        "\r\n",
        "#     x_train = x_train.reshape(50000, 3072)\r\n",
        "#     return (x_train, y_train, x_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KshlXqR83Y_x"
      },
      "source": [
        "# x_train, y_train, x_test, y_test = load_cifar10()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LojulBY3i_M"
      },
      "source": [
        "# print (x_train.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M68ebULXh_Fg"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import os\r\n",
        "import glob\r\n",
        "import helper\r\n",
        "from functools import partial\r\n",
        "import numpy as np\r\n",
        "from tqdm import tqdm\r\n",
        "import cv2\r\n",
        "from skimage import io\r\n",
        "from PIL import Image\r\n",
        "import pickle as pkl\r\n",
        "import scipy.misc\r\n",
        "import random\r\n",
        "import cv2\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "\r\n",
        "from tensorflow.keras.layers import Input\r\n",
        "from tensorflow.keras.models import Model, Sequential\r\n",
        "from tensorflow.keras.layers import Dense, Dropout\r\n",
        "from tensorflow.keras.layers import LeakyReLU\r\n",
        "from tensorflow.keras.optimizers import Adam\r\n",
        "from tensorflow.keras import initializers\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "from tensorflow.keras import backend"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Jm96ptcjxsF"
      },
      "source": [
        "#2.Data preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUOEuZ1llSOF"
      },
      "source": [
        "DATA_BASE_DIR = '/content/drive/My Drive/data/gan_face/data/img_faces_stylegan/'\r\n",
        "image_size = 128\r\n",
        "random_dim = 100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxw-ghLvlmZs"
      },
      "source": [
        "list_ds = tf.data.Dataset.list_files(DATA_BASE_DIR + '/*')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_INr7KLmecB"
      },
      "source": [
        "for f in list_ds.take(5):\r\n",
        "    print(f.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPOy1yWDmjTe"
      },
      "source": [
        "def normalize(image):\r\n",
        "    '''\r\n",
        "        normalizing the images to [-1, 1]\r\n",
        "    '''\r\n",
        "    image = tf.cast(image, tf.float32)\r\n",
        "    image = (image - 127.5) / 127.5\r\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6zYZ0EJm-TG"
      },
      "source": [
        "def augmentation(image):\r\n",
        "    ''' input: image ndarray\r\n",
        "        output: augmanetated data\r\n",
        "        Perform some augmentation\r\n",
        "    '''\r\n",
        "    image = tf.image.random_flip_left_right(image)\r\n",
        "    return image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8d_x3kNKnE0V"
      },
      "source": [
        "def preprocess_image(file_path, target_size=512):\r\n",
        "    ''' input: file path, size of output image\r\n",
        "        output: images\r\n",
        "        Load and resize image from file path\r\n",
        "    '''\r\n",
        "    images = tf.io.read_file(file_path)\r\n",
        "    images = tf.image.decode_jpeg(images, channels=3)\r\n",
        "    images = tf.image.resize(images, (target_size, target_size),\r\n",
        "                           method='nearest', antialias=True)\r\n",
        "    images = augmentation(images)\r\n",
        "    images = normalize(images)\r\n",
        "    return images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0jUTv7X0oOb"
      },
      "source": [
        "files = glob.glob(os.path.join(DATA_BASE_DIR, '*.jpg'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxy9yN570pKU"
      },
      "source": [
        "print (type(files))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjlEKqMW0r9M"
      },
      "source": [
        "image_batch_ath = random.sample(files ,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njTJe01000oK"
      },
      "source": [
        "X = []\r\n",
        "for i in tqdm(range(len(files)),desc=\"load\"):\r\n",
        "  image = cv2.imread(files[i])\r\n",
        "  image = cv2.resize(image,(64,64))\r\n",
        "  image.flatten()\r\n",
        "  X.append(image)        \r\n",
        "X = [np.random.random((3072*4))\r\n",
        "        for i in range(len(X))]\r\n",
        "\r\n",
        "X = np.concatenate([arr[np.newaxis] for arr in X])\r\n",
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D15dWs7O8i9F"
      },
      "source": [
        "print (type(X))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u8XY56kC8pkn"
      },
      "source": [
        "X.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U5mtY_4X6DTR"
      },
      "source": [
        "files = glob.glob(os.path.join(DATA_BASE_DIR, '*.jpg'))\r\n",
        "X = []\r\n",
        "for temp in files:\r\n",
        "    img = cv2.imread(temp)\r\n",
        "    img = cv2.resize(img,(512,512))\r\n",
        "    X.append(img)\r\n",
        "np.array(X)\r\n",
        "\r\n",
        "X_train = X[:2000]\r\n",
        "X_train = [np.random.random((512, 512, 3))\r\n",
        "        for i in range(2000)]\r\n",
        "X_train = np.concatenate([arr[np.newaxis] for arr in X_train])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WRflqvDZ1xkH"
      },
      "source": [
        "#2.Generator And Discriminator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Cw4d2OP0x8x"
      },
      "source": [
        "def get_generator():\r\n",
        "    generator = Sequential()\r\n",
        "    generator.add(Dense(128, input_dim=random_dim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\r\n",
        "    generator.add(LeakyReLU(0.2))\r\n",
        "\r\n",
        "    generator.add(Dense(256))\r\n",
        "    generator.add(LeakyReLU(0.2))\r\n",
        "\r\n",
        "    generator.add(Dense(512))\r\n",
        "    generator.add(LeakyReLU(0.2))\r\n",
        "    generator.add(Dense(1024))\r\n",
        "    generator.add(LeakyReLU(0.2))\r\n",
        "    generator.add(Dense(1024*2))\r\n",
        "    generator.add(LeakyReLU(0.2))\r\n",
        "    generator.add(Dense(1024*4))\r\n",
        "    generator.add(LeakyReLU(0.2))\r\n",
        "    \r\n",
        "    \r\n",
        "    generator.add(Dense(1024*3*4, activation='tanh'))\r\n",
        "    generator.compile(loss='binary_crossentropy', optimizer= Adam( learning_rate=0.0001 , beta_1=0.5 ))\r\n",
        "    return generator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VFBuAFTZ3S04"
      },
      "source": [
        "def get_discriminator():\r\n",
        "    discriminator = Sequential()\r\n",
        "    \r\n",
        "\r\n",
        "    discriminator.add(Dense(1024*4, input_dim=3072*4, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\r\n",
        "    discriminator.add(LeakyReLU(0.2))\r\n",
        "    discriminator.add(Dropout(0.3))\r\n",
        "\r\n",
        "    discriminator.add(Dense(512*4))\r\n",
        "    discriminator.add(LeakyReLU(0.2))\r\n",
        "    discriminator.add(Dropout(0.3))\r\n",
        "\r\n",
        "    discriminator.add(Dense(512*2))\r\n",
        "    discriminator.add(LeakyReLU(0.2))\r\n",
        "    discriminator.add(Dropout(0.3))\r\n",
        "\r\n",
        "    discriminator.add(Dense(512))\r\n",
        "    discriminator.add(LeakyReLU(0.2))\r\n",
        "    discriminator.add(Dropout(0.3))\r\n",
        "\r\n",
        "    discriminator.add(Dense(256))\r\n",
        "    discriminator.add(LeakyReLU(0.2))\r\n",
        "    discriminator.add(Dropout(0.3))\r\n",
        "\r\n",
        "    discriminator.add(Dense(128))\r\n",
        "    discriminator.add(LeakyReLU(0.2))\r\n",
        "    discriminator.add(Dropout(0.3))\r\n",
        "\r\n",
        "    discriminator.add(Dense( 1 , activation='sigmoid'))\r\n",
        "    discriminator.compile( loss='binary_crossentropy' , optimizer=Adam( learning_rate=0.0001 , beta_1=0.5 ))\r\n",
        "    return discriminator"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CIvggjWr4Qmu"
      },
      "source": [
        "def get_gan_network(discriminator, random_dim, generator):\r\n",
        "    # We initially set trainable to False since we only want to train either the\r\n",
        "    # generator or discriminator at a time\r\n",
        "    discriminator.trainable = False\r\n",
        "    # gan input (noise) will be 100-dimensional vectors\r\n",
        "    gan_input = Input(shape=(random_dim,))\r\n",
        "    # the output of the generator (an image)\r\n",
        "    x = generator(gan_input)\r\n",
        "    # get the output of the discriminator (probability if the image is real or not)\r\n",
        "    gan_output = discriminator(x)\r\n",
        "    gan = Model(inputs=gan_input, outputs=gan_output)\r\n",
        "    gan.compile(loss='binary_crossentropy', optimizer=Adam( learning_rate=0.0001 , beta_1=0.5 ))\r\n",
        "    return gan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NH9j2gm04gP4"
      },
      "source": [
        "def plot_generated_images(epoch, generator, examples=100, dim=(50, 50), figsize=(100, 100)):\r\n",
        "    noise = np.random.normal(0, 1, size=[examples, random_dim])\r\n",
        "    generated_images = generator.predict(noise)\r\n",
        "    generated_images = generated_images.reshape(examples, 64, 64, 3)\r\n",
        "\r\n",
        "    plt.figure(figsize=figsize)\r\n",
        "    for i in range(generated_images.shape[0]):\r\n",
        "        plt.subplot(dim[0], dim[1], i+1)\r\n",
        "        plt.imshow(generated_images[i], interpolation='nearest')\r\n",
        "        plt.axis('off')\r\n",
        "    plt.tight_layout()\r\n",
        "    plt.savefig('/content/drive/My Drive/data/gan_face/data/output_gan/gan_generated_image_epoch_%d.png' % epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu6_no2E5ArX"
      },
      "source": [
        "def plot_graph(d_losses,g_losses,epoch):\r\n",
        "  plt.figure(figsize=(10,8))\r\n",
        "  plt.plot(d_losses,label='D loss')\r\n",
        "  plt.plot(g_losses, label='G loss')\r\n",
        "  plt.title('Loss Graph Generator loss vs Discriminator loss')\r\n",
        "  plt.xlabel('Epoch')\r\n",
        "  plt.ylabel('Loss')\r\n",
        "  plt.legend()\r\n",
        "  plt.savefig('/content/drive/My Drive/data/gan_face/data/graph/gan_loss_%d.png' % epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeeCE2O95PDi"
      },
      "source": [
        "def saveModels(generator, discriminator, epoch):\r\n",
        "    generator.save('/content/drive/My Drive/data/gan_face/data/model_gan_t1/gan_generator_epoch_%d.h5' % epoch)\r\n",
        "    discriminator.save('/content/drive/My Drive/data/gan_face/data/model_gan_t1/gan_discriminator_epoch_%d.h5' % epoch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fA1EDbL-jUm"
      },
      "source": [
        "d_losses = []\r\n",
        "g_losses = []"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xto4dU7d5dBz"
      },
      "source": [
        "def train (dataset,epochs=100 , batch_size=128):\r\n",
        "    # Split the training data into batches of size 128\r\n",
        "    #files = glob.glob(os.path.join(data_path, '*.jpg'))\r\n",
        "    x_train = dataset[:2000]\r\n",
        "    batch_count = len(files) / batch_size\r\n",
        "\r\n",
        "    generator = get_generator()\r\n",
        "    discriminator = get_discriminator()\r\n",
        "    gan = get_gan_network(discriminator, random_dim, generator)\r\n",
        "\r\n",
        "    for e in np.arange(1, epochs+1):\r\n",
        "        print ('-'*15, 'Epoch %d' % e, '-'*15)\r\n",
        "        for _ in tqdm(np.arange(batch_count)):\r\n",
        "            # Get a random set of input noise and images\r\n",
        "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\r\n",
        "            '''image_batch_path = random.sample(files,batch_size)\r\n",
        "            image_batch = []\r\n",
        "            for file in image_batch_path:\r\n",
        "              image = cv2.imread(file)\r\n",
        "              image = cv2.resize(image,(32,32))\r\n",
        "              image_batch.append(image)\r\n",
        "\r\n",
        "            image_batch = [np.random.random((3072))\r\n",
        "            for i in range(len(image_batch)) ]\r\n",
        "            image_batch = np.concatenate([arr[np.newaxis] for arr in image_batch])\r\n",
        "            image_batch.reshape(batch_size,3072)'''\r\n",
        "\r\n",
        "              \r\n",
        "            image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size)]\r\n",
        "            # Generate fake CIFAR images\r\n",
        "            generated_images = generator.predict(noise)\r\n",
        "            print (image_batch.shape)\r\n",
        "            X = np.concatenate([image_batch, generated_images])\r\n",
        "\r\n",
        "            # Labels for generated and real data\r\n",
        "            y_dis = np.zeros(2*batch_size)\r\n",
        "            # One-sided label smoothing\r\n",
        "            y_dis[:batch_size] = 0.9\r\n",
        "\r\n",
        "            # Train discriminator\r\n",
        "            discriminator.trainable = True\r\n",
        "            dloss = discriminator.train_on_batch(X, y_dis)\r\n",
        "\r\n",
        "            # Train generator\r\n",
        "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\r\n",
        "            y_gen = np.ones(batch_size)\r\n",
        "            discriminator.trainable = False\r\n",
        "            gloss = gan.train_on_batch(noise, y_gen)\r\n",
        "            print([dloss,gloss])\r\n",
        "            d_losses.append(dloss)\r\n",
        "            g_losses.append(gloss)\r\n",
        "\r\n",
        "        if e == 1 or e % 50 == 0:\r\n",
        "            plot_generated_images(e, generator)\r\n",
        "            plot_graph(d_losses, g_losses,e)\r\n",
        "            saveModels(generator,discriminator,e)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzS0fwiNp-_V"
      },
      "source": [
        "#3.Train\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFmh6FiQ7sQm"
      },
      "source": [
        "if __name__ == '__main__':\r\n",
        "    train(X,200, 256)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}