{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "colab": {
      "name": "PGGAN-Tensorflow.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "_XFumy16U41z"
      ]
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "n7WuGpfyU400"
      },
      "source": [
        "from __future__ import division\n",
        "import os\n",
        "import time\n",
        "import math\n",
        "from functools import partial\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython.display import clear_output\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Layer, InputSpec, Conv2D, Conv2DTranspose, Activation, Reshape, LayerNormalization, BatchNormalization, UpSampling2D\n",
        "from tensorflow.keras.layers import Input, UpSampling2D, Dropout, Concatenate, Add, Dense, Multiply, LeakyReLU, Flatten, AveragePooling2D, Multiply\n",
        "from tensorflow.keras import initializers, regularizers, constraints, Model, Sequential\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.utils import Sequence\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam, SGD\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.initializers import RandomNormal\n",
        "from tensorflow.keras.losses import BinaryCrossentropy\n",
        "from tensorflow.keras.utils import to_categorical, plot_model\n",
        "\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up-4_g_mVkWV",
        "outputId": "aa8b22ef-310c-408e-af2a-7e6a28c50b34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCW94VMEVlo5",
        "outputId": "118dd69b-7e57-46bf-af0f-e497ac3c894a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/drive/My Drive/data/gan_face/data/"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/data/gan_face/data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBWSgR2PX5XK"
      },
      "source": [
        "# !unzip -q \"CelebA-HQ-img\""
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zjtpj1VzU404"
      },
      "source": [
        "MODEL_NAME = 'PGGAN'\n",
        "DATA_BASE_DIR = '/content/drive/My Drive/data/gan_face/data/CelebA-HQ-img' # Modify this to your dataset path.\n",
        "OUTPUT_PATH = '/content/drive/My Drive/data/gan_face/outputs/'\n",
        "MODEL_PATH = '/content/drive/My Drive/data/gan_face/models/'\n",
        "TRAIN_LOGDIR = os.path.join(\"logs\", \"tensorflow\", \"train_data\") # Sets up a log directory.\n",
        "if not os.path.exists(OUTPUT_PATH):\n",
        "    os.makedirs(OUTPUT_PATH)\n",
        "    \n",
        "batch_size = 16\n",
        "# Start from 4 * 4\n",
        "image_size = 4\n",
        "NOISE_DIM = 512\n",
        "LAMBDA = 10\n",
        "\n",
        "EPOCHs = 320\n",
        "CURRENT_EPOCH = 1 # Epoch start from 1. If resume training, set this to the previous model saving epoch.\n",
        "total_data_number = len(os.listdir(DATA_BASE_DIR))\n",
        "\n",
        "# For training speed, this number is lower than the original paper, however the output quality would be lower.\n",
        "switch_res_every_n_epoch = 40\n",
        "#switch_res_every_n_epoch = math.ceil(800000 / total_data_number)\n",
        "\n",
        "SAVE_EVERY_N_EPOCH = 5 # Save checkpoint at every n epoch\n",
        "\n",
        "LR = 1e-3\n",
        "BETA_1 = 0.\n",
        "BETA_2 = 0.99\n",
        "EPSILON = 1e-8\n",
        "# Decay learning rate\n",
        "MIN_LR = 0.000001\n",
        "DECAY_FACTOR=1.00004\n",
        "\n",
        "# Creates a file writer for the log directory.\n",
        "file_writer = tf.summary.create_file_writer(TRAIN_LOGDIR)\n",
        "#test_file_writer = tf.summary.create_file_writer(TEST_LOGDIR)"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8oDk9z9U407",
        "outputId": "02bb0196-1f5f-4d49-9917-2f410e882c43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "list_ds = tf.data.Dataset.list_files(DATA_BASE_DIR + '/*')\n",
        "\n",
        "for f in list_ds.take(5):\n",
        "    print(f.numpy())"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'/content/drive/My Drive/data/gan_face/data/CelebA-HQ-img/20417.jpg'\n",
            "b'/content/drive/My Drive/data/gan_face/data/CelebA-HQ-img/27855.jpg'\n",
            "b'/content/drive/My Drive/data/gan_face/data/CelebA-HQ-img/814.jpg'\n",
            "b'/content/drive/My Drive/data/gan_face/data/CelebA-HQ-img/23958.jpg'\n",
            "b'/content/drive/My Drive/data/gan_face/data/CelebA-HQ-img/5403.jpg'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcr2n-5cU40_"
      },
      "source": [
        "def normalize(image):\n",
        "    '''\n",
        "        normalizing the images to [-1, 1]\n",
        "    '''\n",
        "    image = tf.cast(image, tf.float32)\n",
        "    image = (image - 127.5) / 127.5\n",
        "    return image\n",
        "\n",
        "def augmentation(image):\n",
        "    '''\n",
        "        Perform some augmentation\n",
        "    '''\n",
        "    image = tf.image.random_flip_left_right(image)\n",
        "    return image\n",
        "\n",
        "def preprocess_image(file_path, target_size=512):\n",
        "    images = tf.io.read_file(file_path)\n",
        "    # convert the compressed string to a 3D uint8 tensor\n",
        "    images = tf.image.decode_jpeg(images, channels=3)\n",
        "    images = tf.image.resize(images, (target_size, target_size),\n",
        "                           method='nearest', antialias=True)\n",
        "    images = augmentation(images)\n",
        "    images = normalize(images)\n",
        "    return images"
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jykVjvUYU41C"
      },
      "source": [
        "preprocess_function = partial(preprocess_image, target_size=image_size)\n",
        "train_data = list_ds.map(preprocess_function).shuffle(100).batch(batch_size)"
      ],
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GGQikry7U41F",
        "outputId": "9580b26a-7a46-41e2-e998-01a47dc0e21d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        }
      },
      "source": [
        "sample_img = next(iter(train_data))\n",
        "plt.title('Sample')\n",
        "plt.imshow(np.clip(sample_img[0] * 0.5 + 0.5, 0, 1))"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f940ef1cc18>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 117
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ij9MPCpKU41J"
      },
      "source": [
        "class EqualizeLearningRate(tf.keras.layers.Wrapper):\n",
        "    \"\"\"\n",
        "    Reference from WeightNormalization implementation of TF Addons\n",
        "    EqualizeLearningRate wrapper works for keras CNN and Dense (RNN not tested).\n",
        "    ```python\n",
        "      net = EqualizeLearningRate(\n",
        "          tf.keras.layers.Conv2D(2, 2, activation='relu'),\n",
        "          input_shape=(32, 32, 3),\n",
        "          data_init=True)(x)\n",
        "      net = EqualizeLearningRate(\n",
        "          tf.keras.layers.Conv2D(16, 5, activation='relu'),\n",
        "          data_init=True)(net)\n",
        "      net = EqualizeLearningRate(\n",
        "          tf.keras.layers.Dense(120, activation='relu'),\n",
        "          data_init=True)(net)\n",
        "      net = EqualizeLearningRate(\n",
        "          tf.keras.layers.Dense(n_classes),\n",
        "          data_init=True)(net)\n",
        "    ```\n",
        "    Arguments:\n",
        "      layer: a layer instance.\n",
        "    Raises:\n",
        "      ValueError: If `Layer` does not contain a `kernel` of weights\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, layer, **kwargs):\n",
        "        super(EqualizeLearningRate, self).__init__(layer, **kwargs)\n",
        "        self._track_trackable(layer, name='layer')\n",
        "        self.is_rnn = isinstance(self.layer, tf.keras.layers.RNN)\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        \"\"\"Build `Layer`\"\"\"\n",
        "        input_shape = tf.TensorShape(input_shape)\n",
        "        self.input_spec = tf.keras.layers.InputSpec(\n",
        "            shape=[None] + input_shape[1:])\n",
        "\n",
        "        if not self.layer.built:\n",
        "            self.layer.build(input_shape)\n",
        "\n",
        "        kernel_layer = self.layer.cell if self.is_rnn else self.layer\n",
        "\n",
        "        if not hasattr(kernel_layer, 'kernel'):\n",
        "            raise ValueError('`EqualizeLearningRate` must wrap a layer that'\n",
        "                             ' contains a `kernel` for weights')\n",
        "\n",
        "        if self.is_rnn:\n",
        "            kernel = kernel_layer.recurrent_kernel\n",
        "        else:\n",
        "            kernel = kernel_layer.kernel\n",
        "\n",
        "        # He constant\n",
        "        self.fan_in, self.fan_out= self._compute_fans(kernel.shape)\n",
        "        self.he_constant = tf.Variable(1.0 / np.sqrt(self.fan_in), dtype=tf.float32, trainable=False)\n",
        "\n",
        "        self.v = kernel\n",
        "        self.built = True\n",
        "    \n",
        "    def call(self, inputs, training=True):\n",
        "        \"\"\"Call `Layer`\"\"\"\n",
        "\n",
        "        with tf.name_scope('compute_weights'):\n",
        "            # Multiply the kernel with the he constant.\n",
        "            kernel = tf.identity(self.v * self.he_constant)\n",
        "            \n",
        "            if self.is_rnn:\n",
        "                print(self.is_rnn)\n",
        "                self.layer.cell.recurrent_kernel = kernel\n",
        "                update_kernel = tf.identity(self.layer.cell.recurrent_kernel)\n",
        "            else:\n",
        "                self.layer.kernel = kernel\n",
        "                update_kernel = tf.identity(self.layer.kernel)\n",
        "\n",
        "            # Ensure we calculate result after updating kernel.\n",
        "            with tf.control_dependencies([update_kernel]):\n",
        "                outputs = self.layer(inputs)\n",
        "                return outputs\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return tf.TensorShape(\n",
        "            self.layer.compute_output_shape(input_shape).as_list())\n",
        "    \n",
        "    def _compute_fans(self, shape, data_format='channels_last'):\n",
        "        \"\"\"\n",
        "        From Official Keras implementation\n",
        "        Computes the number of input and output units for a weight shape.\n",
        "        # Arguments\n",
        "            shape: Integer shape tuple.\n",
        "            data_format: Image data format to use for convolution kernels.\n",
        "                Note that all kernels in Keras are standardized on the\n",
        "                `channels_last` ordering (even when inputs are set\n",
        "                to `channels_first`).\n",
        "        # Returns\n",
        "            A tuple of scalars, `(fan_in, fan_out)`.\n",
        "        # Raises\n",
        "            ValueError: in case of invalid `data_format` argument.\n",
        "        \"\"\"\n",
        "        if len(shape) == 2:\n",
        "            fan_in = shape[0]\n",
        "            fan_out = shape[1]\n",
        "        elif len(shape) in {3, 4, 5}:\n",
        "            # Assuming convolution kernels (1D, 2D or 3D).\n",
        "            # TH kernel shape: (depth, input_depth, ...)\n",
        "            # TF kernel shape: (..., input_depth, depth)\n",
        "            if data_format == 'channels_first':\n",
        "                receptive_field_size = np.prod(shape[2:])\n",
        "                fan_in = shape[1] * receptive_field_size\n",
        "                fan_out = shape[0] * receptive_field_size\n",
        "            elif data_format == 'channels_last':\n",
        "                receptive_field_size = np.prod(shape[:-2])\n",
        "                fan_in = shape[-2] * receptive_field_size\n",
        "                fan_out = shape[-1] * receptive_field_size\n",
        "            else:\n",
        "                raise ValueError('Invalid data_format: ' + data_format)\n",
        "        else:\n",
        "            # No specific assumptions.\n",
        "            fan_in = np.sqrt(np.prod(shape))\n",
        "            fan_out = np.sqrt(np.prod(shape))\n",
        "        return fan_in, fan_out"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j73GF8m_U41M"
      },
      "source": [
        "#kernel_initializer = RandomNormal(mean=0.0, stddev=1.0)\n",
        "kernel_initializer = 'he_normal'\n",
        "\n",
        "class PixelNormalization(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "      epsilon: a float-point number, the default is 1e-8\n",
        "    \"\"\"\n",
        "    def __init__(self, epsilon=1e-8):\n",
        "        super(PixelNormalization, self).__init__()\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return inputs / tf.sqrt(tf.reduce_mean(tf.square(inputs), axis=-1, keepdims=True) + self.epsilon)\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return input_shape\n",
        "\n",
        "class MinibatchSTDDEV(tf.keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    Reference from official pggan implementation\n",
        "    https://github.com/tkarras/progressive_growing_of_gans/blob/master/networks.py\n",
        "    \n",
        "    Arguments:\n",
        "      group_size: a integer number, minibatch must be divisible by (or smaller than) group_size.\n",
        "    \"\"\"\n",
        "    def __init__(self, group_size=4):\n",
        "        super(MinibatchSTDDEV, self).__init__()\n",
        "        self.group_size = group_size\n",
        "\n",
        "    def call(self, inputs):\n",
        "        group_size = tf.minimum(self.group_size, tf.shape(inputs)[0])     # Minibatch must be divisible by (or smaller than) group_size.\n",
        "        s = inputs.shape                                             # [NHWC]  Input shape.\n",
        "        y = tf.reshape(inputs, [group_size, -1, s[1], s[2], s[3]])   # [GMHWC] Split minibatch into M groups of size G.\n",
        "        y = tf.cast(y, tf.float32)                              # [GMHWC] Cast to FP32.\n",
        "        y -= tf.reduce_mean(y, axis=0, keepdims=True)           # [GMHWC] Subtract mean over group.\n",
        "        y = tf.reduce_mean(tf.square(y), axis=0)                # [MHWC]  Calc variance over group.\n",
        "        y = tf.sqrt(y + 1e-8)                                   # [MHWC]  Calc stddev over group.\n",
        "        y = tf.reduce_mean(y, axis=[1,2,3], keepdims=True)      # [M111]  Take average over fmaps and pixels.\n",
        "        y = tf.cast(y, inputs.dtype)                                 # [M111]  Cast back to original data type.\n",
        "        y = tf.tile(y, [group_size, s[1], s[2], 1])             # [NHW1]  Replicate over group and pixels.\n",
        "        return tf.concat([inputs, y], axis=-1)                        # [NHWC]  Append as new fmap.\n",
        "    \n",
        "    def compute_output_shape(self, input_shape):\n",
        "        return (input_shape[0], input_shape[1], input_shape[2], input_shape[3] + 1)\n",
        "\n",
        "\n",
        "def upsample_block(x, in_filters, filters, kernel_size=3, strides=1, padding='valid', activation=tf.nn.leaky_relu, name=''):\n",
        "    '''\n",
        "        Upsampling + 2 Convolution-Activation\n",
        "    '''\n",
        "    upsample = UpSampling2D(size=2, interpolation='nearest')(x)\n",
        "    upsample_x = EqualizeLearningRate(Conv2D(filters, kernel_size, strides, padding=padding,\n",
        "                   kernel_initializer=kernel_initializer, bias_initializer='zeros'), name=name+'_conv2d_1')(upsample)\n",
        "    x = PixelNormalization()(upsample_x)\n",
        "    x = Activation(activation)(x)\n",
        "    x = EqualizeLearningRate(Conv2D(filters, kernel_size, strides, padding=padding,\n",
        "                                   kernel_initializer=kernel_initializer, bias_initializer='zeros'), name=name+'_conv2d_2')(x)\n",
        "    x = PixelNormalization()(x)\n",
        "    x = Activation(activation)(x)\n",
        "    return x, upsample\n",
        "\n",
        "def downsample_block(x, filters1, filters2, kernel_size=3, strides=1, padding='valid', activation=tf.nn.leaky_relu, name=''):\n",
        "    '''\n",
        "        2 Convolution-Activation + Downsampling\n",
        "    '''\n",
        "    x = EqualizeLearningRate(Conv2D(filters1, kernel_size, strides, padding=padding,\n",
        "               kernel_initializer=kernel_initializer, bias_initializer='zeros'), name=name+'_conv2d_1')(x)\n",
        "    x = Activation(activation)(x)\n",
        "    x = EqualizeLearningRate(Conv2D(filters2, kernel_size, strides, padding=padding,\n",
        "               kernel_initializer=kernel_initializer, bias_initializer='zeros'), name=name+'_conv2d_2')(x)\n",
        "    x = Activation(activation)(x)\n",
        "    downsample = AveragePooling2D(pool_size=2)(x)\n",
        "\n",
        "    return downsample"
      ],
      "execution_count": 119,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EU9FnI2uU41O"
      },
      "source": [
        "## I used tanh in the G output layers, I didn't test not using activation(linear in the paper) in the output layer, maybe using linear is better."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSxaiK9hU41P"
      },
      "source": [
        "# output_activation = tf.keras.activations.linear\n",
        "output_activation = tf.keras.activations.tanh\n",
        "\n",
        "def generator_input_block(x):\n",
        "    '''\n",
        "        Generator input block\n",
        "    '''\n",
        "    x = EqualizeLearningRate(Dense(4*4*512, kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='g_input_dense')(x)\n",
        "    x = PixelNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Reshape((4, 4, 512))(x)\n",
        "    x = EqualizeLearningRate(Conv2D(512, 3, strides=1, padding='same',\n",
        "                                          kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='g_input_conv2d')(x)\n",
        "    x = PixelNormalization()(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    return x\n",
        "\n",
        "def build_4x4_generator(noise_dim=NOISE_DIM):\n",
        "    '''\n",
        "        4 * 4 Generator\n",
        "    '''\n",
        "    # Initial block\n",
        "    inputs = Input(noise_dim)\n",
        "    x = generator_input_block(inputs)\n",
        "    # Not used in 4 * 4, put it here in order to keep the input here same as the other models\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
        "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(4, 4))\n",
        "    \n",
        "    rgb_out = to_rgb(x)\n",
        "    model = Model(inputs=[inputs, alpha], outputs=rgb_out)\n",
        "    return model\n",
        "\n",
        "def build_8x8_generator(noise_dim=NOISE_DIM):\n",
        "    '''\n",
        "        8 * 8 Generator\n",
        "    '''\n",
        "    # Initial block\n",
        "    inputs = Input(noise_dim)\n",
        "    x = generator_input_block(inputs)\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    \n",
        "    ########################\n",
        "    # Fade in block\n",
        "    ########################\n",
        "    x, up_x = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
        "    \n",
        "    \n",
        "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
        "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(4, 4))\n",
        "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
        "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(8, 8))\n",
        "\n",
        "    l_x = to_rgb(x)\n",
        "    r_x = previous_to_rgb(up_x)\n",
        "    ########################\n",
        "    # Left branch in the paper\n",
        "    ########################\n",
        "    l_x = Multiply()([1 - alpha, l_x])\n",
        "    ########################\n",
        "    # Right branch in the paper\n",
        "    ########################\n",
        "    r_x = Multiply()([alpha, r_x])\n",
        "    combined = Add()([l_x, r_x])\n",
        "    \n",
        "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
        "    return model\n",
        "\n",
        "def build_16x16_generator(noise_dim=NOISE_DIM):\n",
        "    '''\n",
        "        16 * 16 Generator\n",
        "    '''\n",
        "    # Initial block\n",
        "    inputs = Input(noise_dim)\n",
        "    x = generator_input_block(inputs)\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    ########################\n",
        "    # Stable blocks\n",
        "    ########################\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
        "    ########################\n",
        "    # Fade in block\n",
        "    ########################\n",
        "    x, up_x = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
        "    \n",
        "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
        "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(8, 8))\n",
        "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
        "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(16, 16))\n",
        "\n",
        "    l_x = to_rgb(x)\n",
        "    r_x = previous_to_rgb(up_x)\n",
        "    ########################\n",
        "    # Left branch in the paper\n",
        "    ########################\n",
        "    l_x = Multiply()([1 - alpha, l_x])\n",
        "    ########################\n",
        "    # Right branch in the paper\n",
        "    ########################\n",
        "    r_x = Multiply()([alpha, r_x])\n",
        "    combined = Add()([l_x, r_x])\n",
        "    \n",
        "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
        "    return model\n",
        "\n",
        "def build_32x32_generator(noise_dim=NOISE_DIM):\n",
        "    '''\n",
        "        32 * 32 Generator\n",
        "    '''\n",
        "    # Initial block\n",
        "    inputs = Input(noise_dim)\n",
        "    x = generator_input_block(inputs)\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    ########################\n",
        "    # Stable blocks\n",
        "    ########################\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
        "    ########################\n",
        "    # Fade in block\n",
        "    ########################\n",
        "    x, up_x = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(32, 32))\n",
        "    \n",
        "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
        "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(16, 16))\n",
        "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
        "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(32, 32))\n",
        "\n",
        "    l_x = to_rgb(x)\n",
        "    r_x = previous_to_rgb(up_x)\n",
        "    ########################\n",
        "    # Left branch in the paper\n",
        "    ########################\n",
        "    l_x = Multiply()([1 - alpha, l_x])\n",
        "    ########################\n",
        "    # Right branch in the paper\n",
        "    ########################\n",
        "    r_x = Multiply()([alpha, r_x])\n",
        "    combined = Add()([l_x, r_x])\n",
        "    \n",
        "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
        "    return model\n",
        "\n",
        "def build_64x64_generator(noise_dim=NOISE_DIM):\n",
        "    '''\n",
        "        64 * 64 Generator\n",
        "    '''\n",
        "    # Initial block\n",
        "    inputs = Input(noise_dim)\n",
        "    x = generator_input_block(inputs)\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    ########################\n",
        "    # Stable blocks\n",
        "    ########################\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(32, 32))\n",
        "    ########################\n",
        "    # Fade in block\n",
        "    ########################\n",
        "    x, up_x = upsample_block(x, in_filters=512, filters=256, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(64, 64))\n",
        "    \n",
        "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
        "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(32, 32))\n",
        "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
        "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(64, 64))\n",
        "    \n",
        "    l_x = to_rgb(x)\n",
        "    r_x = previous_to_rgb(up_x)\n",
        "    ########################\n",
        "    # Left branch in the paper\n",
        "    ########################\n",
        "    l_x = Multiply()([1 - alpha, l_x])\n",
        "    ########################\n",
        "    # Right branch in the paper\n",
        "    ########################\n",
        "    r_x = Multiply()([alpha, r_x])\n",
        "    combined = Add()([l_x, r_x])\n",
        "    \n",
        "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
        "    return model\n",
        "\n",
        "def build_128x128_generator(noise_dim=NOISE_DIM):\n",
        "    '''\n",
        "        128 * 128 Generator\n",
        "    '''\n",
        "    # Initial block\n",
        "    inputs = Input(noise_dim)\n",
        "    x = generator_input_block(inputs)\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    ########################\n",
        "    # Stable blocks\n",
        "    ########################\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(32, 32))\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=256, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(64, 64))\n",
        "    ########################\n",
        "    # Fade in block\n",
        "    ########################\n",
        "    x, up_x = upsample_block(x, in_filters=256, filters=128, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(128, 128))\n",
        "    \n",
        "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
        "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(64, 64))\n",
        "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
        "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(128, 128))\n",
        "    \n",
        "    l_x = to_rgb(x)\n",
        "    r_x = previous_to_rgb(up_x)\n",
        "    ########################\n",
        "    # Left branch in the paper\n",
        "    ########################\n",
        "    l_x = Multiply()([1 - alpha, l_x])\n",
        "    ########################\n",
        "    # Right branch in the paper\n",
        "    ########################\n",
        "    r_x = Multiply()([alpha, r_x])\n",
        "    combined = Add()([l_x, r_x])\n",
        "    \n",
        "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
        "    return model\n",
        "\n",
        "def build_256x256_generator(noise_dim=NOISE_DIM):\n",
        "    '''\n",
        "        256 * 256 Generator\n",
        "    '''\n",
        "    # Initial block\n",
        "    inputs = Input(noise_dim)\n",
        "    x = generator_input_block(inputs)\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    ########################\n",
        "    # Stable blocks\n",
        "    ########################\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(32, 32))\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=256, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(64, 64))\n",
        "    x, _ = upsample_block(x, in_filters=256, filters=128, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(128, 128))\n",
        "    ########################\n",
        "    # Fade in block\n",
        "    ########################\n",
        "    x, up_x = upsample_block(x, in_filters=128, filters=64, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(256, 256))\n",
        "    \n",
        "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
        "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(128, 128))\n",
        "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
        "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(256, 256))\n",
        "    \n",
        "    l_x = to_rgb(x)\n",
        "    r_x = previous_to_rgb(up_x)\n",
        "    ########################\n",
        "    # Left branch in the paper\n",
        "    ########################\n",
        "    l_x = Multiply()([1 - alpha, l_x])\n",
        "    ########################\n",
        "    # Right branch in the paper\n",
        "    ########################\n",
        "    r_x = Multiply()([alpha, r_x])\n",
        "    combined = Add()([l_x, r_x])\n",
        "    \n",
        "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
        "    return model\n",
        "\n",
        "def build_512x512_generator(noise_dim=NOISE_DIM):\n",
        "    '''\n",
        "        512 * 512 Generator\n",
        "    '''\n",
        "    # Initial block\n",
        "    inputs = Input(noise_dim)\n",
        "    x = generator_input_block(inputs)\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    ########################\n",
        "    # Stable blocks\n",
        "    ########################\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(8, 8))\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(16, 16))\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=512, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(32, 32))\n",
        "    x, _ = upsample_block(x, in_filters=512, filters=256, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(64, 64))\n",
        "    x, _ = upsample_block(x, in_filters=256, filters=128, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(128, 128))\n",
        "    x, _ = upsample_block(x, in_filters=128, filters=64, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(256, 256))\n",
        "    ########################\n",
        "    # Fade in block\n",
        "    ########################\n",
        "    x, up_x = upsample_block(x, in_filters=64, filters=32, kernel_size=3, strides=1,\n",
        "                                         padding='same', activation=tf.nn.leaky_relu, name='Up_{}x{}'.format(512, 512))\n",
        "    \n",
        "    previous_to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
        "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(256, 256))\n",
        "    to_rgb = EqualizeLearningRate(Conv2D(3, kernel_size=1, strides=1,  padding='same', activation=output_activation,\n",
        "                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='to_rgb_{}x{}'.format(512, 512))\n",
        "    \n",
        "    l_x = to_rgb(x)\n",
        "    r_x = previous_to_rgb(up_x)\n",
        "    ########################\n",
        "    # Left branch in the paper\n",
        "    ########################\n",
        "    l_x = Multiply()([1 - alpha, l_x])\n",
        "    ########################\n",
        "    # Right branch in the paper\n",
        "    ########################\n",
        "    r_x = Multiply()([alpha, r_x])\n",
        "    combined = Add()([l_x, r_x])\n",
        "    \n",
        "    model = Model(inputs=[inputs, alpha], outputs=combined)\n",
        "    return model"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh2pzJS7U41R"
      },
      "source": [
        "def discriminator_block(x):\n",
        "    '''\n",
        "        Discriminator output block\n",
        "    '''\n",
        "    x = MinibatchSTDDEV()(x)\n",
        "    x = EqualizeLearningRate(Conv2D(512, 3, strides=1, padding='same',\n",
        "                                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='d_output_conv2d_1')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = EqualizeLearningRate(Conv2D(512, 4, strides=1, padding='valid',\n",
        "                                    kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='d_output_conv2d_2')(x)\n",
        "    x = LeakyReLU()(x)\n",
        "    x = Flatten()(x)\n",
        "    x = EqualizeLearningRate(Dense(1, kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='d_output_dense')(x)\n",
        "    return x\n",
        "\n",
        "def build_4x4_discriminator():\n",
        "    '''\n",
        "        4 * 4 Discriminator\n",
        "    '''\n",
        "    inputs = Input((4,4,3))\n",
        "    # Not used in 4 * 4\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    # From RGB\n",
        "    from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(4, 4))\n",
        "    x = from_rgb(inputs)\n",
        "    x = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='conv2d_up_channel')(x)\n",
        "    x = discriminator_block(x)\n",
        "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
        "    return model\n",
        "\n",
        "def build_8x8_discriminator():\n",
        "    '''\n",
        "        8 * 8 Discriminator\n",
        "    '''\n",
        "    fade_in_channel = 512\n",
        "    inputs = Input((8,8,3))\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    downsample = AveragePooling2D(pool_size=2)\n",
        "    ########################\n",
        "    # Left branch in the paper\n",
        "    ########################\n",
        "    previous_from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(4, 4))\n",
        "    l_x = previous_from_rgb(downsample(inputs))\n",
        "    l_x = Multiply()([1 - alpha, l_x])\n",
        "    ########################\n",
        "    # Right branch in the paper\n",
        "    ########################\n",
        "    from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(8, 8))\n",
        "    r_x = from_rgb(inputs)\n",
        "    ########################\n",
        "    # Fade in block\n",
        "    ########################\n",
        "    r_x = downsample_block(r_x, filters1=512, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
        "    r_x = Multiply()([alpha, r_x])\n",
        "    x = Add()([l_x, r_x])\n",
        "    ########################\n",
        "    # Stable block\n",
        "    ########################\n",
        "    x = discriminator_block(x)\n",
        "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
        "    return model\n",
        "\n",
        "def build_16x16_discriminator():\n",
        "    '''\n",
        "        16 * 16 Discriminator\n",
        "    '''\n",
        "    fade_in_channel = 512\n",
        "    inputs = Input((16, 16, 3))\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    downsample = AveragePooling2D(pool_size=2)\n",
        "    ########################\n",
        "    # Left branch in the paper\n",
        "    ########################\n",
        "    previous_from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(8, 8))\n",
        "    l_x = previous_from_rgb(downsample(inputs))\n",
        "    l_x = Multiply()([1 - alpha, l_x])\n",
        "    ########################\n",
        "    # Right branch in the paper\n",
        "    ########################\n",
        "    from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(16, 16))\n",
        "    r_x = from_rgb(inputs)\n",
        "    ########################\n",
        "    # Fade in block\n",
        "    ########################\n",
        "    r_x = downsample_block(r_x, filters1=512, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
        "    r_x = Multiply()([alpha, r_x])\n",
        "    x = Add()([l_x, r_x])\n",
        "    ########################\n",
        "    # Stable blocks\n",
        "    ########################\n",
        "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
        "    x = discriminator_block(x)\n",
        "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
        "    return model\n",
        "\n",
        "def build_32x32_discriminator():\n",
        "    '''\n",
        "        32 * 32 Discriminator\n",
        "    '''\n",
        "    fade_in_channel = 512\n",
        "    inputs = Input((32, 32, 3))\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    downsample = AveragePooling2D(pool_size=2)\n",
        "    ########################\n",
        "    # Left branch in the paper\n",
        "    ########################\n",
        "    previous_from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(16, 16))\n",
        "    l_x = previous_from_rgb(downsample(inputs))\n",
        "    l_x = Multiply()([1 - alpha, l_x])\n",
        "    ########################\n",
        "    # Right branch in the paper\n",
        "    ########################\n",
        "    from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(32, 32))\n",
        "    r_x = from_rgb(inputs)\n",
        "    ########################\n",
        "    # Fade in block\n",
        "    ########################\n",
        "    r_x = downsample_block(r_x, filters1=512, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(32,32))\n",
        "    r_x = Multiply()([alpha, r_x])\n",
        "    x = Add()([l_x, r_x])\n",
        "    ########################\n",
        "    # Stable blocks\n",
        "    ########################\n",
        "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
        "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
        "    x = discriminator_block(x)\n",
        "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
        "    return model\n",
        "\n",
        "def build_64x64_discriminator():\n",
        "    '''\n",
        "        64 * 64 Discriminator\n",
        "    '''\n",
        "    fade_in_channel = 512\n",
        "    inputs = Input((64, 64, 3))\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    downsample = AveragePooling2D(pool_size=2)\n",
        "    \n",
        "    ########################\n",
        "    # Left branch in the paper\n",
        "    ########################\n",
        "    previous_from_rgb = EqualizeLearningRate(Conv2D(512, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(32, 32))\n",
        "    l_x = previous_from_rgb(downsample(inputs))\n",
        "    l_x = Multiply()([1 - alpha, l_x])\n",
        "    ########################\n",
        "    # Right branch in the paper\n",
        "    ########################\n",
        "    from_rgb = EqualizeLearningRate(Conv2D(256, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(64, 64))\n",
        "    r_x = from_rgb(inputs)\n",
        "    ########################\n",
        "    # Fade in block\n",
        "    ########################\n",
        "    r_x = downsample_block(r_x, filters1=256, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(64,64))\n",
        "    r_x = Multiply()([alpha, r_x])\n",
        "    x = Add()([l_x, r_x])\n",
        "    ########################\n",
        "    # Stable blocks\n",
        "    ########################\n",
        "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(32,32))\n",
        "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
        "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
        "    x = discriminator_block(x)\n",
        "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
        "    return model\n",
        "\n",
        "def build_128x128_discriminator():\n",
        "    '''\n",
        "        128 * 128 Discriminator\n",
        "    '''\n",
        "    fade_in_channel = 256\n",
        "    inputs = Input((128, 128, 3))\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    downsample = AveragePooling2D(pool_size=2)\n",
        "   \n",
        "    ########################\n",
        "    # Left branch in the paper\n",
        "    ########################\n",
        "    previous_from_rgb = EqualizeLearningRate(Conv2D(256, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(64, 64))\n",
        "    l_x = previous_from_rgb(downsample(inputs))\n",
        "    l_x = Multiply()([1 - alpha, l_x])\n",
        "    ########################\n",
        "    # Right branch in the paper\n",
        "    ########################\n",
        "    from_rgb = EqualizeLearningRate(Conv2D(128, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(128, 128))\n",
        "    r_x = from_rgb(inputs)\n",
        "    ########################\n",
        "    # Fade in block\n",
        "    ########################\n",
        "    r_x = downsample_block(r_x, filters1=128, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(128,128))\n",
        "    r_x = Multiply()([alpha, r_x])\n",
        "    x = Add()([l_x, r_x])\n",
        "    ########################\n",
        "    # Stable blocks\n",
        "    ########################\n",
        "    x = downsample_block(x, filters1=256, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(64,64))\n",
        "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(32,32))\n",
        "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
        "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
        "    x = discriminator_block(x)\n",
        "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
        "    return model\n",
        "\n",
        "def build_256x256_discriminator():\n",
        "    '''\n",
        "        256 * 256 Discriminator\n",
        "    '''\n",
        "    fade_in_channel = 128\n",
        "    inputs = Input((256, 256, 3))\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    downsample = AveragePooling2D(pool_size=2)\n",
        "    ########################\n",
        "    # Left branch in the paper\n",
        "    ########################\n",
        "    previous_from_rgb = EqualizeLearningRate(Conv2D(128, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(128, 128))\n",
        "    l_x = previous_from_rgb(downsample(inputs))\n",
        "    l_x = Multiply()([1 - alpha, l_x])\n",
        "    ########################\n",
        "    # Right branch in the paper\n",
        "    ########################\n",
        "    from_rgb = EqualizeLearningRate(Conv2D(64, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(256, 256))\n",
        "    r_x = from_rgb(inputs)\n",
        "    ########################\n",
        "    # Fade in block\n",
        "    ########################\n",
        "    r_x = downsample_block(r_x, filters1=64, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(256,256))\n",
        "    r_x = Multiply()([alpha, r_x])\n",
        "    x = Add()([l_x, r_x])\n",
        "    ########################\n",
        "    # Stable blocks\n",
        "    ########################\n",
        "    x = downsample_block(x, filters1=128, filters2=256, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(128,128))\n",
        "    x = downsample_block(x, filters1=256, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(64,64))\n",
        "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(32,32))\n",
        "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
        "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
        "    x = discriminator_block(x)\n",
        "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
        "    return model\n",
        "\n",
        "def build_512x512_discriminator():\n",
        "    '''\n",
        "        512 * 512 Discriminator\n",
        "    '''\n",
        "    fade_in_channel = 64\n",
        "    inputs = Input((512, 512, 3))\n",
        "    alpha = Input((1), name='input_alpha')\n",
        "    downsample = AveragePooling2D(pool_size=2)\n",
        "    \n",
        "    ########################\n",
        "    # Left branch in the paper\n",
        "    ########################\n",
        "    previous_from_rgb = EqualizeLearningRate(Conv2D(64, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(256, 256))\n",
        "    l_x = previous_from_rgb(downsample(inputs))\n",
        "    l_x = Multiply()([1 - alpha, l_x])\n",
        "    ########################\n",
        "    # Right branch in the paper\n",
        "    ########################\n",
        "    from_rgb = EqualizeLearningRate(Conv2D(32, kernel_size=1, strides=1, padding='same', activation=tf.nn.leaky_relu,\n",
        "                      kernel_initializer=kernel_initializer, bias_initializer='zeros'), name='from_rgb_{}x{}'.format(512, 512))\n",
        "    r_x = from_rgb(inputs)\n",
        "    ########################\n",
        "    # Fade in block\n",
        "    ########################\n",
        "    r_x = downsample_block(r_x, filters1=32, filters2=fade_in_channel, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(512,512))\n",
        "    r_x = Multiply()([alpha, r_x])\n",
        "    x = Add()([l_x, r_x])\n",
        "    ########################\n",
        "    # Stable blocks\n",
        "    ########################\n",
        "    x = downsample_block(x, filters1=64, filters2=128, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(256,256))\n",
        "    x = downsample_block(x, filters1=128, filters2=256, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(128,128))\n",
        "    x = downsample_block(x, filters1=256, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(64,64))\n",
        "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(32,32))\n",
        "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(16,16))\n",
        "    x = downsample_block(x, filters1=512, filters2=512, kernel_size=3, strides=1,\n",
        "                                            padding='same', activation=tf.nn.leaky_relu, name='Down_{}x{}'.format(8,8))\n",
        "    x = discriminator_block(x)\n",
        "    model = Model(inputs=[inputs, alpha], outputs=x)\n",
        "    return model"
      ],
      "execution_count": 121,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tl30Zvs0U41T"
      },
      "source": [
        "def model_builder(target_resolution):\n",
        "    '''\n",
        "        Helper function to build models\n",
        "    '''\n",
        "    generator = None\n",
        "    discriminator = None\n",
        "    if target_resolution == 4:\n",
        "        generator = build_4x4_generator()\n",
        "        discriminator = build_4x4_discriminator()\n",
        "    elif target_resolution == 8:\n",
        "        generator = build_8x8_generator()\n",
        "        discriminator = build_8x8_discriminator()\n",
        "    elif target_resolution == 16:\n",
        "        generator = build_16x16_generator()\n",
        "        discriminator = build_16x16_discriminator()\n",
        "    elif target_resolution == 32:\n",
        "        generator = build_32x32_generator()\n",
        "        discriminator = build_32x32_discriminator()\n",
        "    elif target_resolution == 64:\n",
        "        generator = build_64x64_generator()\n",
        "        discriminator = build_64x64_discriminator()\n",
        "    elif target_resolution == 128:\n",
        "        generator = build_128x128_generator()\n",
        "        discriminator = build_128x128_discriminator()\n",
        "    elif target_resolution == 256:\n",
        "        generator = build_256x256_generator()\n",
        "        discriminator = build_256x256_discriminator()\n",
        "    elif target_resolution == 512:\n",
        "        generator = build_512x512_generator()\n",
        "        discriminator = build_512x512_discriminator()\n",
        "    else:\n",
        "        print(\"target resolution models are not defined yet\")\n",
        "    return generator, discriminator"
      ],
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "rP4HgVfMU41V",
        "outputId": "1cad3722-3392-4f42-ba59-0baf28666a39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 263
        }
      },
      "source": [
        "generator, discriminator = model_builder(image_size)\n",
        "generator.summary()\n",
        "plot_model(generator, show_shapes=True, dpi=64)"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ResourceExhaustedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-123-c055ab5fe309>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_builder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-122-4f5e87de5365>\u001b[0m in \u001b[0;36mmodel_builder\u001b[0;34m(target_resolution)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtarget_resolution\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0mgenerator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_4x4_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0mdiscriminator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_4x4_discriminator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mtarget_resolution\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-80b8e0afa43f>\u001b[0m in \u001b[0;36mbuild_4x4_generator\u001b[0;34m(noise_dim)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# Initial block\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerator_input_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Not used in 4 * 4, put it here in order to keep the input here same as the other models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0malpha\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'input_alpha'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-120-80b8e0afa43f>\u001b[0m in \u001b[0;36mgenerator_input_block\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mGenerator\u001b[0m \u001b[0minput\u001b[0m \u001b[0mblock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     '''\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEqualizeLearningRate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkernel_initializer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias_initializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'zeros'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g_input_dense'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPixelNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_in_functional_construction_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    925\u001b[0m       return self._functional_construction_call(inputs, args, kwargs,\n\u001b[0;32m--> 926\u001b[0;31m                                                 input_list)\n\u001b[0m\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    928\u001b[0m     \u001b[0;31m# Maintains info about the `Layer.call` stack.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_functional_construction_call\u001b[0;34m(self, inputs, args, kwargs, input_list)\u001b[0m\n\u001b[1;32m   1096\u001b[0m         \u001b[0;31m# Build layer if applicable (if the `build` method has been\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0;31m# overridden).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m         \u001b[0mcast_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m_maybe_build\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2641\u001b[0m         \u001b[0;31m# operations.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2642\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_init_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2643\u001b[0;31m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint:disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2644\u001b[0m       \u001b[0;31m# We must set also ensure that the layer is marked as built, and the build\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2645\u001b[0m       \u001b[0;31m# shape is stored since user defined build functions may not be calling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-118-e0395206e86a>\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mkernel_layer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_rnn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/layers/core.py\u001b[0m in \u001b[0;36mbuild\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m   1176\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1178\u001b[0;31m         trainable=True)\n\u001b[0m\u001b[1;32m   1179\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m       self.bias = self.add_weight(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36madd_weight\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint, partitioner, use_resource, synchronization, aggregation, **kwargs)\u001b[0m\n\u001b[1;32m    612\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 614\u001b[0;31m         caching_device=caching_device)\n\u001b[0m\u001b[1;32m    615\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mregularizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    616\u001b[0m       \u001b[0;31m# TODO(fchollet): in the future, this should be handled at the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_add_variable_with_custom_getter\u001b[0;34m(self, name, shape, dtype, initializer, getter, overwrite, **kwargs_for_getter)\u001b[0m\n\u001b[1;32m    748\u001b[0m         \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0minitializer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 750\u001b[0;31m         **kwargs_for_getter)\n\u001b[0m\u001b[1;32m    751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    752\u001b[0m     \u001b[0;31m# If we set an initializer and the variable processed it, tracking will not\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer_utils.py\u001b[0m in \u001b[0;36mmake_variable\u001b[0;34m(name, shape, dtype, initializer, trainable, caching_device, validate_shape, constraint, use_resource, collections, synchronization, aggregation, partitioner)\u001b[0m\n\u001b[1;32m    143\u001b[0m       \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m       shape=variable_shape if variable_shape else None)\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariableV1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 260\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v1_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    261\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_variable_v1_call\u001b[0;34m(cls, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint, use_resource, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m    219\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m   def _variable_v2_call(cls,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    197\u001b[0m                         shape=None):\n\u001b[1;32m    198\u001b[0m     \u001b[0;34m\"\"\"Call on Variable class. Useful to force the signature.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m       \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0msynchronization\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msynchronization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2596\u001b[0m         \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2597\u001b[0;31m         shape=shape)\n\u001b[0m\u001b[1;32m   2598\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2599\u001b[0m     return variables.RefVariable(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_v2_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVariableMetaclass\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, variable_def, import_scope, constraint, distribute_strategy, synchronization, aggregation, shape)\u001b[0m\n\u001b[1;32m   1516\u001b[0m           \u001b[0maggregation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maggregation\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m           \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m           distribute_strategy=distribute_strategy)\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m   def _init_from_args(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, caching_device, name, dtype, constraint, synchronization, aggregation, distribute_strategy, shape)\u001b[0m\n\u001b[1;32m   1649\u001b[0m           \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1650\u001b[0m             initial_value = ops.convert_to_tensor(\n\u001b[0;32m-> 1651\u001b[0;31m                 \u001b[0minitial_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0minit_from_fn\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0minitial_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1652\u001b[0m                 name=\"initial_value\", dtype=dtype)\n\u001b[1;32m   1653\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/initializers/initializers_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    395\u001b[0m        \u001b[0;34m(\u001b[0m\u001b[0mvia\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_floatx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m     \"\"\"\n\u001b[0;32m--> 397\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVarianceScaling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_get_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, shape, dtype)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;31m# constant from scipy.stats.truncnorm.std(a=-2, b=2, loc=0., scale=1.)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m       \u001b[0mstddev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m.87962566103423978\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_random_generator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstddev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    556\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"untruncated_normal\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m       \u001b[0mstddev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops_v2.py\u001b[0m in \u001b[0;36mtruncated_normal\u001b[0;34m(self, shape, mean, stddev, dtype)\u001b[0m\n\u001b[1;32m   1051\u001b[0m       \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtruncated_normal\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1052\u001b[0m     return op(\n\u001b[0;32m-> 1053\u001b[0;31m         shape=shape, mean=mean, stddev=stddev, dtype=dtype, seed=self.seed)\n\u001b[0m\u001b[1;32m   1054\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1055\u001b[0m \u001b[0;31m# Compatibility aliases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/random_ops.py\u001b[0m in \u001b[0;36mtruncated_normal\u001b[0;34m(shape, mean, stddev, dtype, seed, name)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0mseed1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_seed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     rnd = gen_random_ops.truncated_normal(\n\u001b[0;32m--> 196\u001b[0;31m         shape_tensor, dtype, seed=seed1, seed2=seed2)\n\u001b[0m\u001b[1;32m    197\u001b[0m     \u001b[0mmul\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnd\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mstddev_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_random_ops.py\u001b[0m in \u001b[0;36mtruncated_normal\u001b[0;34m(shape, dtype, seed, seed2, name)\u001b[0m\n\u001b[1;32m    900\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    901\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 902\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    903\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[512,8192] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc [Op:TruncatedNormal]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "biNB_XeIU41Y"
      },
      "source": [
        "discriminator.summary()\n",
        "plot_model(discriminator, show_shapes=True, dpi=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8UHLn7uBU41b"
      },
      "source": [
        "D_optimizer = Adam(learning_rate=LR, beta_1=BETA_1, beta_2=BETA_2, epsilon=EPSILON)\n",
        "G_optimizer = Adam(learning_rate=LR, beta_1=BETA_1, beta_2=BETA_2, epsilon=EPSILON)\n",
        "\n",
        "def learning_rate_decay(current_lr, decay_factor=DECAY_FACTOR):\n",
        "    new_lr = max(current_lr / decay_factor, MIN_LR)\n",
        "    return new_lr\n",
        "\n",
        "def set_learning_rate(new_lr, D_optimizer, G_optimizer):\n",
        "    '''\n",
        "        Set new learning rate to optimizers\n",
        "    '''\n",
        "    K.set_value(D_optimizer.lr, new_lr)\n",
        "    K.set_value(G_optimizer.lr, new_lr)\n",
        "    \n",
        "def calculate_batch_size(image_size):\n",
        "    if image_size < 64:\n",
        "        return 16\n",
        "    elif image_size < 128:\n",
        "        return 12\n",
        "    elif image_size == 128:\n",
        "        return 8\n",
        "    elif image_size == 256:\n",
        "        return 4\n",
        "    else:\n",
        "        return 3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UryoVuhxU41e"
      },
      "source": [
        "def generate_and_save_images(model, epoch, test_input, figure_size=(12,6), subplot=(3,6), save=True, is_flatten=False):\n",
        "    # Test input is a list include noise and label\n",
        "    predictions = model.predict(test_input)\n",
        "    fig = plt.figure(figsize=figure_size)\n",
        "    for i in range(predictions.shape[0]):\n",
        "        axs = plt.subplot(subplot[0], subplot[1], i+1)\n",
        "        plt.imshow(predictions[i] * 0.5 + 0.5)\n",
        "        plt.axis('off')\n",
        "    if save:\n",
        "        plt.savefig(os.path.join(OUTPUT_PATH, '{}x{}_image_at_epoch_{:04d}.png'.format(predictions.shape[1], predictions.shape[2], epoch)))\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7TX4G8kYU41h"
      },
      "source": [
        "num_examples_to_generate = 9\n",
        "\n",
        "# We will reuse this seed overtime (so it's easier)\n",
        "# to visualize progress in the animated GIF)\n",
        "sample_noise = tf.random.normal([num_examples_to_generate, NOISE_DIM], seed=0)\n",
        "sample_alpha = np.repeat(1, num_examples_to_generate).reshape(num_examples_to_generate, 1).astype(np.float32)\n",
        "generate_and_save_images(generator, 0, [sample_noise, sample_alpha], figure_size=(6,6), subplot=(3,3), save=False, is_flatten=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrnPM3rmU41k"
      },
      "source": [
        "## Training steps\n",
        "\n",
        "Adding tf function decorator in training functions can speed up training, but it cannot work with auto growing network. Which means you have to re-run the notebook, adjust the hyper parameters and set new current epoch to start with the higher resolution."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9y-upZSU41k"
      },
      "source": [
        "#@tf.function\n",
        "def WGAN_GP_train_d_step(generator, discriminator, real_image, alpha, batch_size, step):\n",
        "    '''\n",
        "        One training step\n",
        "        \n",
        "        Reference: https://www.tensorflow.org/tutorials/generative/dcgan\n",
        "    '''\n",
        "    noise = tf.random.normal([batch_size, NOISE_DIM])\n",
        "    epsilon = tf.random.uniform(shape=[batch_size, 1, 1, 1], minval=0, maxval=1)\n",
        "    ###################################\n",
        "    # Train D\n",
        "    ###################################\n",
        "    with tf.GradientTape(persistent=True) as d_tape:\n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            fake_image = generator([noise, alpha], training=True)\n",
        "            fake_image_mixed = epsilon * tf.dtypes.cast(real_image, tf.float32) + ((1 - epsilon) * fake_image)\n",
        "            fake_mixed_pred = discriminator([fake_image_mixed, alpha], training=True)\n",
        "            \n",
        "        # Compute gradient penalty\n",
        "        grads = gp_tape.gradient(fake_mixed_pred, fake_image_mixed)\n",
        "        grad_norms = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "        gradient_penalty = tf.reduce_mean(tf.square(grad_norms - 1))\n",
        "        \n",
        "        fake_pred = discriminator([fake_image, alpha], training=True)\n",
        "        real_pred = discriminator([real_image, alpha], training=True)\n",
        "        \n",
        "        D_loss = tf.reduce_mean(fake_pred) - tf.reduce_mean(real_pred) + LAMBDA * gradient_penalty\n",
        "    # Calculate the gradients for discriminator\n",
        "    D_gradients = d_tape.gradient(D_loss,\n",
        "                                            discriminator.trainable_variables)\n",
        "    # Apply the gradients to the optimizer\n",
        "    D_optimizer.apply_gradients(zip(D_gradients,\n",
        "                                                discriminator.trainable_variables))\n",
        "    # Write loss values to tensorboard\n",
        "    if step % 10 == 0:\n",
        "        with file_writer.as_default():\n",
        "            tf.summary.scalar('D_loss', tf.reduce_mean(D_loss), step=step)\n",
        "            \n",
        "#@tf.function\n",
        "def WGAN_GP_train_g_step(generator, discriminator, alpha, batch_size, step):\n",
        "    '''\n",
        "        One training step\n",
        "        \n",
        "        Reference: https://www.tensorflow.org/tutorials/generative/dcgan\n",
        "    '''\n",
        "    noise = tf.random.normal([batch_size, NOISE_DIM])\n",
        "    ###################################\n",
        "    # Train G\n",
        "    ###################################\n",
        "    with tf.GradientTape() as g_tape:\n",
        "        fake_image = generator([noise, alpha], training=True)\n",
        "        fake_pred = discriminator([fake_image, alpha], training=True)\n",
        "        G_loss = -tf.reduce_mean(fake_pred)\n",
        "    # Calculate the gradients for discriminator\n",
        "    G_gradients = g_tape.gradient(G_loss,\n",
        "                                            generator.trainable_variables)\n",
        "    # Apply the gradients to the optimizer\n",
        "    G_optimizer.apply_gradients(zip(G_gradients,\n",
        "                                                generator.trainable_variables))\n",
        "    # Write loss values to tensorboard\n",
        "    if step % 10 == 0:\n",
        "        with file_writer.as_default():\n",
        "            tf.summary.scalar('G_loss', G_loss, step=step)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nYvKYc6SU41m"
      },
      "source": [
        "math.ceil(total_data_number / batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTy_llPaU41p"
      },
      "source": [
        "# Load previous resolution model\n",
        "if image_size > 4:\n",
        "    if os.path.isfile(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(int(image_size / 2), int(image_size / 2)))):\n",
        "        generator.load_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(int(image_size / 2), int(image_size / 2))), by_name=True)\n",
        "        print(\"generator loaded\")\n",
        "    if os.path.isfile(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(int(image_size / 2), int(image_size / 2)))):\n",
        "        discriminator.load_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(int(image_size / 2), int(image_size / 2))), by_name=True)\n",
        "        print(\"discriminator loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HXwTyEdU41s"
      },
      "source": [
        "# To resume training, comment it if not using.\n",
        "if os.path.isfile(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(int(image_size), int(image_size)))):\n",
        "    generator.load_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(int(image_size), int(image_size))), by_name=False)\n",
        "    print(\"generator loaded\")\n",
        "if os.path.isfile(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(int(image_size), int(image_size)))):\n",
        "    discriminator.load_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(int(image_size), int(image_size))), by_name=False)\n",
        "    print(\"discriminator loaded\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "PH1c8tSPU41v"
      },
      "source": [
        "current_learning_rate = LR\n",
        "training_steps = math.ceil(total_data_number / batch_size)\n",
        "# Fade in half of switch_res_every_n_epoch epoch, and stablize another half\n",
        "alpha_increment = 1. / (switch_res_every_n_epoch / 2 * training_steps)\n",
        "alpha = min(1., (CURRENT_EPOCH - 1) % switch_res_every_n_epoch * training_steps *  alpha_increment)\n",
        "\n",
        "for epoch in range(CURRENT_EPOCH, EPOCHs + 1):\n",
        "    start = time.time()\n",
        "    print('Start of epoch %d' % (epoch,))\n",
        "    print('Current alpha: %f' % (alpha,))\n",
        "    print('Current resolution: {} * {}'.format(image_size, image_size))\n",
        "    # Using learning rate decay\n",
        "#     current_learning_rate = learning_rate_decay(current_learning_rate)\n",
        "#     print('current_learning_rate %f' % (current_learning_rate,))\n",
        "#     set_learning_rate(current_learning_rate) \n",
        "    \n",
        "    for step, (image) in enumerate(train_data):\n",
        "        current_batch_size = image.shape[0]\n",
        "        alpha_tensor = tf.constant(np.repeat(alpha, current_batch_size).reshape(current_batch_size, 1), dtype=tf.float32)\n",
        "        # Train step\n",
        "        \n",
        "        WGAN_GP_train_d_step(generator, discriminator, image, alpha_tensor,\n",
        "                             batch_size=tf.constant(current_batch_size, dtype=tf.int64), step=tf.constant(step, dtype=tf.int64))\n",
        "        WGAN_GP_train_g_step(generator, discriminator, alpha_tensor,\n",
        "                             batch_size=tf.constant(current_batch_size, dtype=tf.int64), step=tf.constant(step, dtype=tf.int64))\n",
        "        \n",
        "        \n",
        "        # update alpha\n",
        "        alpha = min(1., alpha + alpha_increment)\n",
        "        \n",
        "        if step % 10 == 0:\n",
        "            print ('.', end='')\n",
        "    \n",
        "    # Clear jupyter notebook cell output\n",
        "    clear_output(wait=True)\n",
        "    # Using a consistent image (sample_X) so that the progress of the model is clearly visible.\n",
        "    generate_and_save_images(generator, epoch, [sample_noise, sample_alpha], figure_size=(6,6), subplot=(3,3), save=True, is_flatten=False)\n",
        "    \n",
        "    if epoch % SAVE_EVERY_N_EPOCH == 0:\n",
        "        generator.save_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(image_size, image_size)))\n",
        "        discriminator.save_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(image_size, image_size)))\n",
        "        print ('Saving model for epoch {}'.format(epoch))\n",
        "    \n",
        "    print ('Time taken for epoch {} is {} sec\\n'.format(epoch,\n",
        "                                                      time.time()-start))\n",
        "    \n",
        "    \n",
        "    # Train next resolution\n",
        "    if epoch % switch_res_every_n_epoch == 0:\n",
        "        print('saving {} * {} model'.format(image_size, image_size))\n",
        "        generator.save_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(image_size, image_size)))\n",
        "        discriminator.save_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(image_size, image_size)))\n",
        "        # Reset alpha\n",
        "        alpha = 0\n",
        "        previous_image_size = int(image_size)\n",
        "        image_size = int(image_size * 2)\n",
        "        if image_size > 512:\n",
        "            print('Resolution reach 512x512, finish training')\n",
        "            break\n",
        "        print('creating {} * {} model'.format(image_size, image_size))\n",
        "        generator, discriminator = model_builder(image_size)\n",
        "        generator.load_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(previous_image_size, previous_image_size)), by_name=True)\n",
        "        discriminator.load_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(previous_image_size, previous_image_size)), by_name=True)\n",
        "        \n",
        "        print('Making {} * {} dataset'.format(image_size, image_size))\n",
        "        batch_size = calculate_batch_size(image_size)\n",
        "        preprocess_function = partial(preprocess_image, target_size=image_size)\n",
        "        train_data = list_ds.map(preprocess_function).shuffle(100).batch(batch_size)\n",
        "        training_steps = math.ceil(total_data_number / batch_size)\n",
        "        alpha_increment = 1. / (switch_res_every_n_epoch / 2 * training_steps)\n",
        "        print('start training {} * {} model'.format(image_size, image_size))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XFumy16U41z"
      },
      "source": [
        "## Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kbfK12r9U41z"
      },
      "source": [
        "generator, discriminator = model_builder(128)\n",
        "generator.load_weights(os.path.join(MODEL_PATH, '{}x{}_generator.h5'.format(128, 128)))\n",
        "discriminator.load_weights(os.path.join(MODEL_PATH, '{}x{}_discriminator.h5'.format(128, 128)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "_5gNEUnmU411"
      },
      "source": [
        "test_noise = tf.random.normal([9, NOISE_DIM])\n",
        "test_alpha = tf.ones([9, 1])\n",
        "#generate_and_save_images(generator, 0, [test_noise], figure_size=(12,6), subplot=(3,6), save=False, is_flatten=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy2z8-PuU413"
      },
      "source": [
        "prediction = generator.predict([test_noise, test_alpha])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KKkAfWdgU416"
      },
      "source": [
        "def image_grid(images, fig):\n",
        "    \"\"\"Return a 3x3 grid of the MNIST images as a matplotlib figure.\"\"\"\n",
        "    # Create a figure to contain the plot.\n",
        "    for i in range(9):\n",
        "        # Start next subplot.\n",
        "        axs = fig.add_subplot(3, 3, i + 1)\n",
        "        axs.set_xticks([])\n",
        "        axs.set_yticks([])\n",
        "        axs.imshow(np.clip(images[i] * 0.5 + 0.5, 0, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQOhOX8wU419"
      },
      "source": [
        "# Plot the real images\n",
        "fig1 = plt.figure(figsize=(9,9))\n",
        "image_grid(sample_img.numpy(), fig1)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwqQxGlEU42A"
      },
      "source": [
        "# Plot the fake images from the last epoch\n",
        "fig2 = plt.figure(figsize=(9,9))\n",
        "image_grid(prediction, fig2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLv_9oraU42C"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}